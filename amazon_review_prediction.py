# -*- coding: utf-8 -*-
"""Amazon Review Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FXIPgyCrPTlGjAvCJDrYbc4lTHTV45eh

Importing the electronics dataset from the Amazon review dataset. The smaller dataset consisting of 12169 reviews were imported because of computational constraints
"""

!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software_5.json.gz

"""Importing all the dependencies"""

# Commented out IPython magic to ensure Python compatibility.
import os
import json
import gzip
import pandas as pd
import matplotlib
# %matplotlib inline
import seaborn
from urllib.request import urlopen
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
from wordcloud import WordCloud
import matplotlib.pyplot as plt

"""Loading the JSON data and subsequently converting it into a pandas dataframe"""

data = []
with gzip.open('Software_5.json.gz') as f:
    for l in f:
        data.append(json.loads(l.strip()))

data_df = pd.DataFrame.from_dict(data)

"""### Dropping all columns which are irrelavant. 
Also labelling anything greater than or equal to 3star as 0 and lesser than that as 1. 
"""

#joining the two text columns
data_df['text'] =  data_df['reviewText'] + data_df['summary']

data_df['is_bad_review'] = data_df['overall'].apply(lambda x:1 if x < 3 else 0)  

data_df.drop(columns = ['vote', 'image','verified','reviewerID','unixReviewTime','style','reviewerName','reviewTime','asin','reviewText','summary'],axis=1,inplace=True)
data_df.head()

"""#Ratings Distribution"""

data_df.groupby('overall').text.count().plot.bar(figsize=(10,5), title='Ratings Distribution', colormap='tab20')
plt.xlabel("Ratings")
plt.ylabel("Frequency")
plt.show()

"""###Checking the ratio of positive to negative reviews in total


"""

data_df["is_bad_review"].value_counts(normalize = True)

"""#Text Preprocessing"""

#text pre-processing

from nltk.corpus import stopwords    
from textblob import Word
stoplist = set(stopwords.words('english'))

def cleaning_text(df):
    df['text'] = df['text'].str.lower().str.replace('[^\w\s]','')                                                               # Punctuation Removal
    df['text'] = df['text'].apply( lambda x: ' '.join([w for w in str(x).split() if w not in stoplist]) )                       # Removing stopwords
    df['text'] = df['text'].apply( lambda x:' '.join(x for x in x.split(" ") if not any( c.isdigit() for c in x) ) )             # Numbers Removal
    df['text'] = df['text'].apply( lambda x:' '.join(x for x in x.split(" ") if len(x) > 1) )                               # Single letter Words Removal
    df['text'] = df['text'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))                        # Lemmatization

cleaning_text(data_df)

"""Adding number of words and columns and integrating them as features"""

data_df['nb_chars'] = data_df['text'].apply(lambda x: len(str(x)))

data_df['nb_words'] = data_df['text'].apply(lambda x: len(str(x).split(" ")))

"""#TF-IDF column"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(min_df = 10)
tfidf_result = tfidf.fit_transform(data_df["text"]).toarray()
tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())
tfidf_df.columns = ["word_" + str(x) for x in tfidf_df.columns]
tfidf_df.index = data_df.index

data_df = pd.concat([data_df, tfidf_df], axis=1)

"""#Visualizing the words using WordCloud"""

#WordCloud of the reviews
wordcloud = WordCloud(background_color='white',max_words = 200,  max_font_size = 40,  scale = 3,   random_state = 42).generate(' '.join(data_df['text']))  

fig = plt.figure(1, figsize = (10, 10))
fig.suptitle(None, fontsize = 10)
fig.subplots_adjust(top = 2.3)

plt.axis("off")
plt.imshow(wordcloud, interpolation='bilinear')
plt.show()

"""#Sentiment Analysis using SentimentIntensityAnalyser from NLTK"""

from nltk.sentiment.vader import SentimentIntensityAnalyzer


sid = SentimentIntensityAnalyzer()
data_df['sentiment_score'] = data_df['text'].apply(lambda x: sid.polarity_scores(x))
data_df = pd.concat([data_df.drop(['sentiment_score'], axis=1), data_df['sentiment_score'].apply(pd.Series)], axis=1)

"""#Visualization of Compound Score plotted over 2 lines - the Good reviews and the Bad reviews"""

#Distribution of compound score
import seaborn as sns

for x in [0, 1]:
    subset = data_df[data_df['is_bad_review'] == x]
    
    # Density plot of data set
    if x == 0:
        label = "Good reviews"
        #sns.kdeplot(subset['compound'],label ='Good Reviews').set_xlim(-1,1)
        sns.kdeplot(subset['compound'],label ='Good Reviews')
    else:
        label = "Bad reviews"
        #sns.kdeplot(subset['compound'],label = 'Bad Reviews').set_xlim(-1,1)
        sns.kdeplot(subset['compound'],label = 'Bad Reviews')
        
    plt.ylabel('Density in Thousands')
    
    plt.legend()
    plt.title("Density plot for compound score")

"""#Classification of reviews using various models -



*   Gaussian Naive Bayes
*   Decision Tree Classifier
*   Random Forest Classifier


"""

import time
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.metrics import plot_confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_curve
import sklearn.metrics as metrics
from time import process_time

ignore_cols = ['is_bad_review', 'overall', 'text']
features = [c for c in data_df.columns if c not in ignore_cols]

#Splitting the dataset into train and test 80-20
X_train, X_test, y_train, y_test = train_test_split(data_df[features], data_df['is_bad_review'], test_size = 0.20, random_state = 42)

models = []
models.append(('RandomForestClassifier', RandomForestClassifier()))
models.append(('GaussianNB',GaussianNB()))
models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))
accuracy=[]
time=[]
label=[]
for i,j in models:
   t=process_time()
   k=j.fit(X_train,y_train)
   y_test_pred = j.predict(X_test)
   z=(process_time() - t)
   score = metrics.accuracy_score(y_test, y_test_pred)
   accuracy.append(score*100)
   label.append(i)
   time.append(z)

"""#Comparsion of accuracy percentage by various classifiers"""

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
barlist=ax.bar(label,accuracy)
plt.ylabel('Accuracy',fontsize=20)
plt.show()
# Accuracy shown by various algorithms-> [0.9000016298320968, 0.6110894181960171, 0.8438110113237017]

"""#Comparision of time taken in seconds by various classifiers

"""

#Comparision of time taken in seconds by various classifiers
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
barlist=ax.bar(label,time)
plt.ylabel('Time in seconds',fontsize=20)
plt.show()
# Time taken by various algorithms ->[28.581550722999992, 0.9800993559999966, 28.451397748999995]

"""#Choosing Random Forest Classifier as our classification Algorithm"""

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

#Predicting the sentiment for the test-set
y_test_pred = rfc.predict(X_test)


#classification report
print(classification_report(y_test,y_test_pred))

"""#Feature Importance"""

#Show feature importance
feature_importances_df = pd.DataFrame({"feature": features, "importance": rfc.feature_importances_}).sort_values("importance", ascending = False)
feature_importances_df.head(10)

"""###ROC Curve Plot"""

#ROC curve plot 
metrics.plot_roc_curve(rfc, X_test,y_test)
plt.show()

"""### Final Decision Path of the algorithm

"""

# final decision path of the algorithm
rfc.decision_path(X_train)

"""###Confusion Matrix"""

plot_confusion_matrix(rfc, X_test, y_test)  
plt.show()
